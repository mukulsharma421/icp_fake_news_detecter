# %% [markdown]
# @Mukul sharma
# mukulsharma9988a@gmail.com
# Github:mukulsharma421
# 

# %% [markdown]
# **IMPORTING LIBRARIES**

# %%
import pandas as pd
import sklearn
import itertools
import numpy as np
import seaborn as sb
import re
import nltk
import pickle
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from matplotlib import pyplot as plt
from sklearn.linear_model import PassiveAggressiveClassifier
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

# %% [markdown]
# **DATA PREPERATION**

# %%
#data collection
#loading dataset into pandas dataframe
train_df = pd.read_csv(r'dataset/train.csv')

# %%
train_df.head(15)

# %%
train_df = train_df.drop("author", axis = 1)
train_df = train_df.drop("title", axis = 1)
train_df = train_df.drop("id", axis = 1)

# %%
train_df.shape

# %%
train_df.head(15)

# %%
train_df.isna().sum()

# %%
def create_distribution(dataFile):
    return sb.countplot(x='label', data=dataFile, palette='hls')

# by calling below we can see that training, test and valid data seems to be failry evenly distributed between the classes
create_distribution(train_df)

# %%
def data_qualityCheck():
    print("Checking data qualitites...")
    train_df.isnull().sum()
    train_df.info()  
    print("check finished.")
data_qualityCheck()

# %%
train_df = train_df.dropna()

# %%
data_qualityCheck()

# %%
train_df.shape

# %%
train_df.head(10)

# %%
train_df.reset_index(drop= True,inplace=True)

# %%
train_df.head(10)

# %%
label_train = train_df.label

# %%
label_train.head(10)

# %%
train_df = train_df.drop("label", axis = 1)

# %%
train_df.head(10)

# %%
lemmatizer = WordNetLemmatizer()
stpwrds = list(stopwords.words('english'))

# %%
stpwrds

# %%


# %%

for x in range(len(train_df)) :
    corpus = []
    review = train_df['text'][x]
    review = re.sub(r'[^a-zA-Z\s]', '', review)
    review = review.lower()
    review = nltk.word_tokenize(review)
    for y in review :
        if y not in stpwrds :
            corpus.append(lemmatizer.lemmatize(y))
    review = ' '.join(corpus)
    train_df['text'][x] = review      

# %%
train_df['text'][2182]

# %%


# %%

X_train, X_test, Y_train, Y_test = train_test_split(train_df['text'], label_train, test_size=0.2, random_state=0)

# %%
X_train

# %%
X_train.shape

# %%
Y_train

# %%


# %%
#converting to textual data into numerical data/feature vector

tfidf_v = TfidfVectorizer()
tfidf_X_train = tfidf_v.fit_transform(X_train)
tfidf_X_test = tfidf_v.transform(X_test)

# %%
print(tfidf_X_train,tfidf_X_test)

# %%
tfidf_X_train.shape

# %%
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# %%
#model and model training

classifier = PassiveAggressiveClassifier()
classifier.fit(tfidf_X_train,Y_train)

# %%
#accuracy score on the test data
Y_pred = classifier.predict(tfidf_X_test)
score = metrics.accuracy_score(Y_test, Y_pred)
print(f'Accuracy: {round(score*100,2)}%')
cm = metrics.confusion_matrix(Y_test, Y_pred)
plot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data'])

# %%
print(classification_report(Y_test, Y_pred))

# %% [markdown]
# 

# %%
#pickling the model into disk
pickle.dump(classifier,open('./model.pkl', 'wb'))
pickle.dump(tfidf_v,open('vector.pkl', 'wb'))

# %%
# load the model from disk
loaded_model = pickle.load(open('./model.pkl', 'rb'))
#vector_form = pickle.load(open('vector.pkl', 'rb'))

# %%
def fake_news_det(news):
    review = news
    review = re.sub(r'[^a-zA-Z\s]', '', review)
    review = review.lower()
    review = nltk.word_tokenize(review)
    corpus = []
    for y in review :
        if y not in stpwrds :
            corpus.append(lemmatizer.lemmatize(y))     
    input_data = [' '.join(corpus)]
    vectorized_input_data = tfidf_v.transform(input_data)
    prediction = loaded_model.predict(vectorized_input_data)
    if prediction[0] == 1:
        print("Prediction of the News :  Looking Fakeâš  NewsðŸ“° ")
    else:
        print("Prediction of the News : Looking Real NewsðŸ“° ")
      

# %%
X_test

# %%
X_test[16236]

# %%
Y_test[16236]

# %%
news=str(input("Enter the News:"))
fake_news_det(news)

# %%


# %%
print(tfidf_X_test)

# %%
print(Y_test)

# %%
X_new=tfidf_X_test[3]
prediction = loaded_model.predict(X_new)

if (prediction[0] == 1):
	print("Prediction of the News :  Looking Fakeâš  NewsðŸ“° ")
else:
	print("Prediction of the News : Looking Real NewsðŸ“° ")

# %%
print(Y_test[17543])

# %%


# %%


# %%


# %%


# %%



